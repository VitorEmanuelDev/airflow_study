Project Setup and Execution

This document provides instructions on how to set up and run the Airflow project.

Prerequisites

Ensure the following are installed on your system:

Docker

Docker Compose

Python 3.x

Project Structure

The project has the following key directories and files:

dags/: Contains all the DAG definitions

plugins/: For custom Airflow plugins.

logs/: Stores logs generated by Airflow.

.env: Environment variables for configuring the Airflow setup.

.gitignore: Specifies files and directories to ignore in Git.

docker-compose.yaml: Configuration file to set up Airflow using Docker Compose.

Setup Instructions

Clone the Repository

git clone <repository_url>
cd <repository_name>

Set Up Environment Variables
Ensure the .env file exists in the root directory. Add necessary environment variables like:

AIRFLOW_UID=<any_int_number>

Initialize Airflow
Run the following command to initialize the Airflow environment:

docker-compose up -d

Start Airflow
Bring up the Airflow services (webserver, scheduler, etc.) using Docker Compose:

docker-compose up

This will start the Airflow webserver and scheduler. By default, the webserver should be accessible at:
http://localhost:8081

Access Airflow
Open your browser and go to http://localhost:8081. Use the default credentials:

Username: airflow

Password: airflow

Adding New DAGs

Place your DAG Python files in the dags/ directory.

Airflow will automatically detect and register the DAGs.

Troubleshooting

Log Files: Check logs in the logs/ directory for debugging issues.

Reset Airflow Environment:
If you encounter issues, reset the environment:

docker-compose down --volumes --remove-orphans
docker-compose up -d

Stopping the Project

To stop the running Airflow instance, run:

docker-compose down

Documentation:

https://airflow.apache.org/docs/apache-airflow/stable/index.html

